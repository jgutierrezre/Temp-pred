{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font color= F30720> <b> <i> Predicción de la temperatura promedio en Bogotá a partir de redes neuronales </i> </b> </font>"
      ],
      "metadata": {
        "id": "rMtBflBAGLaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <img src=\"https://cdn.pixabay.com/photo/2013/07/13/10/23/temperature-157127_1280.png?raw=true\\\"width='' height='250'> </center>\n",
        "Fuente: <a href='https://cdn.pixabay.com/photo/2013/07/13/10/23/temperature-157127_1280.png'>pixabay.com</a>"
      ],
      "metadata": {
        "id": "CROSFd1hG-oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color= 199EDC> <b> Introducción  </b> </font>"
      ],
      "metadata": {
        "id": "lGSXQbsiGeTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las redes neuronales han tenido un desenvolvimiento bastante óptimo en las útimas décadas, además con el rápido avance e innovación de la tecnología se ha logrado el mejoramiento de diversas tareas a través de la automatización de funciones que se van mejorando a partir del aprendizaje automático. Dichas tareas en el pasado necesitarían de uno o más individuos para su realización, sin embargo, en el contexto de redes neuronales estas simplemente deben ser programadas de manera adecuada, además del ingreso de valores de entrada para su ejecuación. Por lo tanto, si le red esta trabajando correctamente, los valores correspondientes a las salidas serán lo más óptimos y acertados posibles.\n",
        "\n",
        "En cuanto a ventajas, las redes neuronales son capaces de entrenarse, auto organizarse, aprender y olvidar, también son flexibles, por lo tanto son capaces de adaptarse a nuevos ambientes. Asimismo son robustas y tolerantes a fallas, por lo que el fallo en una neurona o nodo no implica el fallo en toda la red, lo cual permite que se adapten a modelos más complejos a los conocidos normalmente, además son buenas en el reconocimiento de patrones y asociaciones entre individuos de los conjuntos de datos.\n",
        "\n",
        "Una de las principales desventajas en cuanto al manejo de redes neuronales es cuando el conjunto de datos no cuenta con una cantidad adecuada de entradas para el correcto entrenamiento de la red, lo cual puede provocar salidas erróneas.  "
      ],
      "metadata": {
        "id": "6uxK6G1J2Qi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color= 199EDC> <b> Redes neuronales artificiales</b> </font>\n",
        "#### <font color= blue>  ¿Qué son? </font>"
      ],
      "metadata": {
        "id": "RbV-tg10Gk2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelos computacionales basados en procesos biologicos, apoyandose en la arquitectura del cerebro (capas), a través de estructuras llamadas nodos y neuronas, las cuales mediante conexiones (sinapsis) posibilitan el flujo de información. Permiten realizar actividades como clustering, reconocimiento de voz e imagenes, entre otras tareas de forma que la asistencia humana sea mínima, debido a que son capaces de modelar e interpretar relaciones y patrones de un conjunto de información de entrada.\n",
        "<center> <img src=\"https://cdn.pixabay.com/photo/2018/06/27/12/55/artificial-neural-network-3501528_1280.png?raw=true\\\"width='' height='250'> </center>\n",
        "Fuente: <a href='https://pixabay.com/es/illustrations/red-neuronal-artificial-ana-3501528/'>pixabay.com</a>\n",
        "\n",
        "#### <font color= blue>  ¿Cómo funcionan? </font>\n",
        "A pesar de que el término <i> red neuronal </i> suena bastante complejo la idea de su funcionamiento es bastante sencilla, esta basada en el funcionamiento del cerebro humano, inicialmente se dan valores de entrada, los cuales llegan a un nodo llamado neurona, donde se les pondera por un peso, finalmente estos valores se procesan a partir de una función de activación que nos genera un valor de salida o predicción. Al comienzo los pesos pueden ser aleatorios, por lo tanto generar salidas que no se asocian a los resultados reales, sin embargo, a medida que la red va recibiendo un dato de entrada adicional se va entrenando y mejorando los resultados hasta cumplir con el objetivo de alguna regla de decisión seleccionada previamente, dicha regla de decisión es evaluada para cada dato de entrada y así la red va asociando valores tanto a los pesos como a las salidas que cuanndo hay más información si el algoritmo empleado es correcto va a ser más precisa. \n",
        "\n",
        "**Perceptrón:** Unidad de medida de una red neuronal, encargada de efectúar cálculos para detectar características o tendencias en los datos de entrada, permiten que las neuronas artificiales aprendan y traten los elementos de una serie de datos.\n",
        "\n",
        "Descrito a partir de un enfoque matemático, el dato de salida esta dado por:\n",
        "$$y_i = f \\left( ∑w_ix_i + b \\right) $$\n",
        "donde $f$ corresponde a la función de activación, $w_i$ a las ponderaciones, $x_i$ los datos de entrada y $b$ el sesgo\n",
        "\n",
        "<center> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Perceptr%C3%B3n_5_unidades.svg/400px-Perceptr%C3%B3n_5_unidades.svg.png?raw=true\\\" width='' height='400'> </center>\n",
        "Fuente: <a href='https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Perceptr%C3%B3n_5_unidades.svg/400px-Perceptr%C3%B3n_5_unidades.svg.png'>upload.wikimedia.org</a>\n",
        "\n",
        "#### <font color= blue>  Tipos </font>\n",
        "Existen varios tipos de redes neuronales, como son las monocapa o perceptrón simple, perceptrón multicapa (MLP), convolucionales (CNN), redes neuronales recurrentes (RNN), de retroalimentación o redes de base radial (RBF). Además también se pueden clasificar según el método de aprendizaje que se tenga, como como aprendizaje supervisado, aprendizaje por corrección de error, estocástico, aprendizaje autosupervisado, hebbiano, competitivo y comparativo, o por último, aprendizaje por refuerzo. A continuación se presentan una red monocapa, una multicapa y una recurrente.\n",
        "\n",
        "<center> <img src=\"https://www.researchgate.net/profile/Lino-Manjarrez/publication/315762548/figure/fig5/AS:479196280561668@1491260704310/Figura-III6-a-Red-Neuronal-monocapa-b-Red-Neuronal-multicapa-de-propagacion-hacia.png?raw=true\\\" width='' height=''> </center>\n",
        "Fuente: <a href='https://www.researchgate.net/profile/Lino-Manjarrez/publication/315762548/figure/fig5/AS:479196280561668@1491260704310/Figura-III6-a-Red-Neuronal-monocapa-b-Red-Neuronal-multicapa-de-propagacion-hacia.png'>www.researchgate.net</a>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "El estudio de la temperatura promedio en Bogotá, se centrará en redes de aprendizaje supervisado, principalmente en redes multicapa y recurrentes."
      ],
      "metadata": {
        "id": "NxR_uZRMdjsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color= 199EDC> <b> Redes neuronales Recurrentes</b> </font>"
      ],
      "metadata": {
        "id": "nReOl8_5Grd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basadas en el trabajo de David Rumelhart en 1986 y un tipo especial de red recurrente, denominadas *Redes de Hopfield*, descubiertas por John Hopfield en 1982. Las cuales poseen sistemas de memoria asociativa con unidades binaria y están diseñadas para converger a un mínimo local, pero la convergencia a uno de los patrones almacenados no está garantizada.\n",
        "\n",
        "#### <font color= blue>  ¿Qué son? </font>\n",
        "\n",
        "Redes neuronales que permiten permiten analizar datos secuenciales, es decir que no se supone la independencia entre los datos de entrada sin importar la dimensionalidad, por lo tanto, analizan un conjunto de datos que tiene sentido siempre y cuando este se analice en conjunto, como por ejemplo un video que esta constituido por una secuencia de imagenes o un texto a partir de secuencias de palabras.\n",
        "\n",
        "Uno de los atributos más definitorios de las RNN es la compartición de sus parámetros. Sin\n",
        "compartir parámetros, el modelo asignaría parámetros únicos para representar a cada dato en una\n",
        "secuencia y, por lo tanto, no podría realizar inferencias sobre secuencias de longitud variable\n",
        "\n",
        "<center> <img src=\"https://github.com/LauraCRincon/ProyectoFinal/blob/main/Imagenes/RedRecurrente.png?raw=true\" width='' height=''> </center>\n",
        "Fuente: <a href='https://www.deeplearningbook.org/front_matter.pdf'>Deep Learning,  </a> Ian Goodfellow, Yoshua Bengio y Aaron Courville\n",
        "\n",
        "La relación de recurencia viene dada por\n",
        "$$\n",
        "\\left.s^{(t)}=f\\left(s^{t-1}\\right)\\right)\n",
        "$$\n",
        "lo cual indica que el estado del sistema que depende de un paso de tiempo anterior dado por t -1.\n",
        "Esta ecuación puede reescribirse como $h^{(t)}$\n",
        "$$h^{(t)}=f\\left(h^{t-1}, x^{(t)}\\right)$$\n",
        "donde $x^{(t)}$ es la entrada de una instancia de tiempo en particular. La importancia de $h^{(t)}$ es que\n",
        "es una representación de los aspectos relevantes de la secuencia pasada de entradas hasta t.\n",
        "\n",
        "#### <font color= blue>  Topologías de redes neuronales recurrentes </font>\n",
        "Existen 3 tipos\n",
        "\n",
        "- **sequence-vector:** Entrada de un conjunto de datos secuenciales y la salida es un dato, por ejemplo la clasificación de sentimientos.\n",
        "<center> <img src=\"https://abdatum.com/media/images/arquitectura-many-to-one.jpeg?raw=true\\\" width='600' height=''> </center>\n",
        "Fuente: <a href='https://abdatum.com/media/images/arquitectura-many-to-one.jpeg'>abdatum.com</a>\n",
        "- **sequence-sequence:** Tanto entrada como salida son conjuntos de secuencias, como por ejemplo los traductores automáticos.\n",
        "<center> <img src=\"https://abdatum.com/media/images/arquitectura-many-to-many.jpeg?raw=true\\\" width='600' height=''> </center>\n",
        "Fuente: <a href='https://abdatum.com/media/images/arquitectura-many-to-many.jpeg'>abdatum.com</a>\n",
        "- **vector-sequence:** Entrada de un dato y la salida datos secuenciales, como por ejemplo ingresar una palabra y esta produzca una descripción.\n",
        "<center> <img src=\"https://abdatum.com/media/images/arquitectura-one-to-many.jpeg?raw=true\\\" width='600' height=''> </center>\n",
        "Fuente: <a href='https://abdatum.com/media/images/arquitectura-one-to-many.jpeg'>abdatum.com</a>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "npZloyDAPRqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color= 199EDC> <b> Redes LSTM (Long Short Term Memory Networks)</b> </font>"
      ],
      "metadata": {
        "id": "6BB-gHMDG0pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redes neuronales Long short-term memory (LSTM) fueron inventadas por Hochreiter y Schmidhuber en 1997 las cuales consisten en unas celdas de memoria que permiten a la red recordar valores por períodos cortos o largos y establecieron récords de eficiencia en distintos ámbitos de aplicación.\n",
        "\n",
        "Vimos que una Red Recurrente convencional tiene una memoria de corto plazo, resultado de las múltiples transformaciones que sufre el estado oculto a su paso por la red. Las Redes LSTM resuelven este problema usando el concepto de la celda de estado (que es la memoria de la red). En estas Redes LSTM es posible agregar o eliminar información a esta celda de estado usando una serie de compuertas (forget, update y output) que permiten discriminar entre la información relevante e irrelevante.\n",
        "\n",
        "Esto hace que las Redes LSTM sean mucho más robustas que las redes recurrentes convencionales, pues como su nombre lo indica poseen una memoria tanto de largo (long) como de corto (short) plazo.\n",
        "\n",
        "#### <font color= blue>  celdas LSTM </font>\n",
        "\n",
        "Comparado con una celda de red recurrente básica, la celda LSTM tiene una entrada y una salida adicional. Este elemento adicional se conoce como celda de estado.\n",
        "Esta celda de estado es la clave del funcionamiento de las Redes LSTM. Funciona como una banda transportadora a la que se pueden añadir o de donde se pueden remover datos que no queremos que queden en la memoria de la red:\n",
        "\n",
        "#### <font color= blue> compuertas </font>\n",
        "\n",
        "Para añadir o remover datos de esta memoria usamos varias compuertas: forget gate (que permite eliminar elementos de la memoria), la update gate (que permite añadir nuevos elementos a la memoria) y la compuerta de salida (que permite crear el estado oculto actualizado)\n",
        "\n",
        "<center> <img src=\"https://www.codificandobits.com/img/posts/2019-07-20/compuertas-red-lstm.gif?raw=true\"> </center>\n",
        "Fuente: <a href='https://www.codificandobits.com/'> www.codificandobits.com </a>\n",
        "\n",
        "Estas compuertas son redes neuronales que funcionan como válvulas: totalmente abiertas permiten el paso de información, y totalmente cerradas lo bloquean por completo.\n",
        "\n",
        "Cada una de estas compuertas (o válvulas) está conformada por tres elementos: una red neuronal, una función sigmoidal y un elemento multiplicador.\n",
        "\n",
        "La función sigmoidal es precisamente la que da a la compuerta el comportamiento de válvula, pues al alcanzar valores entre 0 y 1 permite anular por completo los valores de entrada (si la salida es 0, lo que equivale a una válvula cerrada) y permitir el paso de los mismos (si la salida es 1, lo que equivale a una válvula totalmente abierta):\n",
        "\n",
        "<center> <img src=\"https://www.codificandobits.com/img/posts/2019-07-20/elementos-compuerta-red-lstm.png?raw=true\"> </center>\n",
        "Fuente: <a href='https://www.codificandobits.com/'> www.codificandobits.com </a>\n",
        "\n",
        "La **compuerta forget** permite decidir qué información se va a descartar, y que por tanto no pasará a la celda de estado.\n",
        "\n",
        "Para ello toma el estado oculto anterior y la entrada actual, los transforma y los lleva a la función de activación sigmoidal. Los coeficientes ($W_f$ y $b_f$) se aprenden durante el entrenamiento, y como salida genera el vector $f_t$:\n",
        "\n",
        "<center> <img src=\"https://www.codificandobits.com/img/posts/2019-07-20/compuerta-forget.png?raw=true\"> </center>\n",
        "Fuente: <a href='https://www.codificandobits.com/'> www.codificandobits.com </a>\n",
        "\n",
        "Si uno de los valores de este vector es 0 (o cercano a 0) entonces la LSTM eliminará esa porción de información, mientras que si alcanza valores iguales (o cercanos) a 1 esta información se mantendrá y llegará a la celda de estado.\n",
        "\n",
        "La **update gate**, que como su nombre lo indica nos permite actualizar la memoria de la celda LSTM y decide como se se subirán los nuevos valores al estado de la celda..\n",
        "\n",
        "Para ello, tomamos nuevamente el estado oculto anterior y la entrada actual, los transformamos y los llevamos de nuevo a una función de activación sigmoidal. También en este caso, los coeficientes ($W_s$ y $b_s$) se aprenden durante el entrenamiento, y como salida esta compuerta genera el vector $u_t$:\n",
        "\n",
        "<center> <img src=\"https://www.codificandobits.com/img/posts/2019-07-20/compuerta-update.png?raw=true\"> </center>\n",
        "Fuente: <a href='https://www.codificandobits.com/'> www.codificandobits.com </a>\n",
        "\n",
        "En este caso, los valores que queremos preservar en la memoria de la red serán aquellos cercanos a 1.\n",
        "\n",
        "Teniendo ya los datos generados por las compuertas forget y update, podemos ahora sí actualizar la celda de estado (es decir la memoria de la red LSTM).\n",
        "\n",
        "\n",
        "#### <font color= blue> Actualización de la celda de estado </font>\n",
        "\n",
        "En primer lugar eliminamos la información irrelevante de la celda de estado, multiplicando el valor anterior de esta celda por el vector generado por la compuerta forget\n",
        "\n",
        "A continuación creamos un vector de valores candidatos a formar parte de la nueva memoria. De nuevo, los parámetros Wc y bc se aprenden durante el entrenamiento. Ahora filtramos estos valores, multiplicando punto a punto el vector que acabamos de obtener con el generado por la compuerta “update”, y el resultado lo sumamos a los valores anteriores de la celda de estado, generando así la memoria actualizada:\n",
        "\n",
        "\n",
        "<center> <img src=\"https://github.com/LauraCRincon/ProyectoFinal/blob/main/Imagenes/actualizacion-celda-estado.gif?raw=true\"> </center>\n",
        "Fuente: <a href='https://www.codificandobits.com/'> www.codificandobits.com </a>\n",
        "\n",
        "\n",
        "#### <font color= blue> Cálculo del nuevo estado oculto </font>\n",
        "\n",
        "Finalmente debemos calcular el nuevo estado oculto, para lo cual usamos la **output gate** o compuerta de salida.\n",
        "Este estado oculto de salida es simplemente una versión filtrada del estado de la celda que acabamos de generar.\n",
        "\n",
        "En primer lugar escalamos el nuevo **cell state** para garantizar que esté en el rango de -1 a 1 (el rango que tiene precisamente el estado oculto). Para ello usamos la función tangente hiperbólica. Ahora, usamos la compuerta de salida para determinar qué porciones del cell-state entrarán a formar parte del nuevo estado oculto. Al igual que en los casos anteriores, los parámetros $W_o$ y $b_o$ serán aprendidos durante el entrenamiento. Y finalmente, filtramos los valores del cell-state con el vector generado por la compuerta de salida:\n",
        "\n",
        "\n",
        "<center> <img src=\"https://github.com/LauraCRincon/ProyectoFinal/blob/main/Imagenes/calculo-estado-oculto.gif?raw=true\"> </center>\n",
        "Fuente: <a href='https://www.codificandobits.com/'> www.codificandobits.com </a>\n",
        "\n",
        "\n",
        "\n",
        "#### <font color= blue> Ventaja de las Redes LSTM </font>\n",
        "\n",
        "Cuando analizamos una secuencia realmente tenemos réplicas de la celda LSTM vistas anteriormente, cada una de ellas correspondiente a un instante de tiempo diferente dentro de la secuencia.\n",
        "\n",
        "Acá se evidencia claramente el concepto del cell state como “banda transportadora”: la información puede ser fácilmente removida o añadida de esta memoria: basta con entrenar adecuadamente las compuertas forget y update:\n",
        "\n",
        "\n",
        "<center> <img src=\"https://www.codificandobits.com/img/posts/2019-07-20/red-lstm-ventaja-celda-de-estados.png?raw=true\"> </center>\n",
        "Fuente: <a href='https://www.codificandobits.com/'> www.codificandobits.com </a>"
      ],
      "metadata": {
        "id": "nBTPqnwTpk3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color= 199EDC> <b> Redes GRU (Gated Recurrent Unit Networks)</b> </font>"
      ],
      "metadata": {
        "id": "kMSKcfPa9yQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Son un tipo de red neuronal recurrente que utiliza unidades recurrentes cerradas en lugar de las unidades ocultas tradicionales. Los GRU RNN fueron propuestos por primera vez en 2014 por Kyunghyun Cho et al. como una forma de mejorar la arquitectura de RNNs existentes. La principal ventaja de las GRU sobre las RNN tradicionales es que son mejores al capturar dependencias a largo plazo. Esto se debe al hecho de que las unidades recurrentes cerradas permiten que la información pase más fácilmente entre las unidades ocultas y las unidades de salida.\n",
        "\n",
        "#### <font color= blue> Celda GRU </font>\n",
        "\n",
        "Las GRU son similares a las RNN tradicionales en que operan en una secuencia de vectores de entrada, pero difieren en la forma en que propagan la información a través de la red. Las RNN tradicionales mantienen un único vector de estado oculto, que se actualiza en cada paso de tiempo de la secuencia. Las GRU, por otro lado, tienen dos vectores de estado ocultos: un vector de \"memoria\" que se actualiza cada paso de tiempo y un vector \"oculto\" que se restablece al vector de memoria cada vez que se procesa un nuevo vector de entrada.\n",
        "\n",
        "<center> <img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\"> </center>\n",
        "Fuente: <a href='http://colah.github.io'> colah.github.io </a>\n",
        "\n",
        "#### <font color= blue> Compuertas </font>\n",
        "\n",
        "Una GRU tiene dos puertas, una puerta de reinicio y una puerta de actualización. La puerta de actualización decide qué parte del estado de celda anterior conservar y cuánto actualizar. La puerta de reinicio decide cuánto del estado de celda anterior olvidar y cuánto recordar.\n",
        "\n",
        "#### <font color= blue> Ventaja Redes GRU </font>\n",
        "\n",
        "Los dos vectores de estado ocultos permiten que las GRU modelen mejor las dependencias a largo plazo que las RNN tradicionales. El vector de memoria se encarga de almacenar información sobre el pasado, mientras que el vector oculto se encarga de procesar nueva información. Esto significa que el vector oculto puede olvidar información que ya no es relevante, mientras que el vector de memoria puede realizar un seguimiento de las dependencias a largo plazo. Por lo tanto, a menudo se utilizan en aplicaciones como el procesamiento del lenguaje natural y la previsión de series temporales.\n",
        "\n",
        "<center> <img src=\"https://dennybritz.com/wp-content/uploads/2015/10/Screen-Shot-2015-10-23-at-10.36.51-AM.png\"> </center>\n",
        "Fuente: <a href='https://dennybritz.com'> dennybritz.com </a>\n",
        "\n",
        "\n",
        "#### <font color= blue> Diferencias con LSTMs </font>\n",
        "\n",
        "La idea básica de usar un mecanismo de activación para aprender dependencias a largo plazo es la misma que en un LSTM, pero hay algunas diferencias clave:\n",
        "\n",
        "- La principal diferencia entre una GRU y una LSTM es que una GRU solo tiene dos puertas, mientras que una LSTM tiene tres.\n",
        "- Las GRU tampoco tienen una memoria interna separada del estado oculto expuesto y no tienen una puerta de salida.\n",
        "- Las puertas de entrada y de olvido en una GRU están acopladas por una puerta de actualización, y la puerta de reinicio se aplica directamente al estado oculto anterior. Entonces, la puerta de reinicio en un LSTM realmente se divide en ambas cosas.\n",
        "No se aplica una segunda no linealidad al calcular la salida."
      ],
      "metadata": {
        "id": "N6AV0KvJ_NjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color= 199EDC> <b> Referencias </b> </font>"
      ],
      "metadata": {
        "id": "7ADk8C9b942q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-4/\n",
        "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "* https://www.youtube.com/watch?v=8HyCNIVRbSU"
      ],
      "metadata": {
        "id": "NZKegGJVDP4e"
      }
    }
  ]
}